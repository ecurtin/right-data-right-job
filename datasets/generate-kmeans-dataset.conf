spark-bench = {
  spark-submit-parallel = false
  spark-submit-config = [{
    spark-home = "/opt/spark-dists/spark-2.2.0-bin-without-hadoop"
    spark-args = {
      master = "spark://devnexus-master:7077"
      executor-memory = "15g"
      total-executor-cores = "70"
    }
    workload-suites = [
      {
        descr = "Generating CSV of kmeans data"
        benchmark-output = "hdfs://devnexus/data-generation.csv"
        save-mode = "append"
        workloads = [
          {
            name = "data-generation-kmeans"
            rows = 1651527663
            cols = 25
            save-mode = "ignore"
            output = "hdfs:///devnexus/kmeans-data.csv"
          }
        ]
      }
    ]
  },
    {
      spark-home = "/opt/spark-dists/spark-2.2.0-bin-without-hadoop"
      spark-args = {
        master = "spark://devnexus-master:7077"
        executor-memory = "15g"
        total-executor-cores = "70"
      }
      conf = {
        "spark.sql.parquet.compression.codec" = "uncompressed"
      }
      workload-suites = [
        {
          descr = "Read in CSV, write as Uncompressed Parquet"
          benchmark-output = "hdfs://devnexus/data-generation.csv"
          save-mode = "append"
          workloads = [
            {
              name = "sql"
              query = "select * from input"
              save-mode = "ignore"
              input = "hdfs:///devnexus/kmeans-data.csv"
              output = "hdfs:///devnexus/kmeans-data-uncompressed.parquet"
            }
          ]
        }
      ]
    },
    {
      spark-home = "/opt/spark-dists/spark-2.2.0-bin-without-hadoop"
      spark-args = {
        master = "spark://devnexus-master:7077"
        executor-memory = "15g"
        total-executor-cores = "70"
      }
      conf = {
        "spark.sql.parquet.compression.codec" = "gzip"
      }
      workload-suites = [
        {
          descr = "Read in CSV, write as GZIP Parquet"
          benchmark-output = "hdfs://devnexus/data-generation.csv"
          save-mode = "append"
          workloads = [
            {
              name = "sql"
              query = "select * from input"
              save-mode = "ignore"
              input = "hdfs:///devnexus/kmeans-data.csv"
              output = "hdfs:///devnexus/kmeans-data-gzip.parquet"
            }
          ]
        }
      ]
    },
    {
      spark-home = "/opt/spark-dists/spark-2.2.0-bin-without-hadoop"
      spark-args = {
        master = "spark://devnexus-master:7077"
        executor-memory = "15g"
        total-executor-cores = "70"
      }
      conf = {
        "spark.sql.parquet.compression.codec" = "snappy"
      }
      workload-suites = [
        {
          descr = "Read in CSV, write as Snappy Parquet"
          benchmark-output = "hdfs://devnexus/data-generation.csv"
          save-mode = "append"
          workloads = [
            {
              name = "sql"
              query = "select * from input"
              save-mode = "ignore"
              input = "hdfs:///devnexus/kmeans-data.csv"
              output = "hdfs:///devnexus/kmeans-data-snappy.parquet"
            }
          ]
        }
      ]
    },
    {
      spark-home = "/opt/spark-dists/spark-2.2.0-bin-without-hadoop"
      spark-args = {
        master = "spark://devnexus-master:7077"
        executor-memory = "15g"
        total-executor-cores = "70"
      }
      workload-suites = [
        {
          descr = "Read in CSV, write as ORC"
          benchmark-output = "hdfs://devnexus/data-generation.csv"
          save-mode = "append"
          workloads = [
            {
              name = "sql"
              query = "select * from input"
              save-mode = "ignore"
              input = "hdfs:///devnexus/kmeans-data.csv"
              output = "hdfs:///devnexus/kmeans-data-orc.orc"
            }
          ]
        }
      ]
    }]
}
